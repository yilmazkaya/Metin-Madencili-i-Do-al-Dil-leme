{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a63d10",
   "metadata": {},
   "source": [
    "# BOW (Bag Of Words): Kelime Çantası\n",
    "# Kelime çantası modeli doğal dil işleme ve enformasyon getiriminde kullanılan basitleştirici bir temsildir. Bu modelde bir metin (cümle ya da belge gibi bir metin) kelimelerinin çantası (çoklukümesi) halinde temsil edilir, çoksallık tutulurken gramer ve hatta kelime sırası gözardı edilir. Kelime çantası modeli bilgisayarla görmede de kullanılmıştır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439ebe3",
   "metadata": {},
   "source": [
    "# Kelime çantası modeli belge sınıflandırma yöntemlerinde yaygınca kullanılır: her kelimenin oluşu (sıklığı) bir sınıflandırıcının eğitilmesinde özellik olarak kullanılır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3010d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Çantası Matrisi:\n",
      "[[0 1 0 0 1 1 0 0]\n",
      " [0 0 0 1 1 1 0 0]\n",
      " [1 0 2 0 0 0 1 1]]\n",
      "\n",
      "Kelimelers:\n",
      "['bugün' 'example' 'hava' 'second' 'sentence' 'short' 'yağışlı' 'çok']\n",
      "\n",
      "Orjinal Dökümanlar:\n",
      "- an example of a short sentence\n",
      "- a second short sentence\n",
      "- bugün hava hava çok yağışlı\n",
      "\n",
      "Tokenize Dökümanlr\n",
      "- example short sentence\n",
      "- second short sentence\n",
      "- bugün hava hava çok yağışlı\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Örnek Cümleler\n",
    "documents = [\n",
    "    \"an example of a short sentence\",\n",
    "    \"a second short sentence\",\n",
    "    \"bugün hava hava çok yağışlı\"\n",
    "]\n",
    "\n",
    "# Tokenize ve Stopwords\n",
    "def tokenize_and_remove_stopwords(document):\n",
    "    words = word_tokenize(document)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Tonenize ve stopword işlemlerin her dökümana uygulanması\n",
    "tokenized_documents = [' '.join(tokenize_and_remove_stopwords(doc)) for doc in documents]\n",
    "\n",
    "# CountVectorizer ile kelime çantalarının oluşturulması\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(tokenized_documents)\n",
    "\n",
    "# Kelime çantası matrisi\n",
    "bow_array = bag_of_words.toarray()\n",
    "\n",
    "# Matrisin gösterimi\n",
    "print(\"Kelime Çantası Matrisi:\")\n",
    "print(bow_array)\n",
    "\n",
    "# Keliemler\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Kelimelerin gözterilmesi kelime çantasından\n",
    "print(\"\\nKelimelers:\")\n",
    "print(feature_names)\n",
    "\n",
    "#Orjinal dökümanlar\n",
    "print(\"\\nOrjinal Dökümanlar:\")\n",
    "for doc in documents:\n",
    "    print(f\"- {doc}\")\n",
    "\n",
    "# Tokenie edilmiş dökümanlar\n",
    "print(\"\\nTokenize Dökümanlr\")\n",
    "for doc in tokenized_documents:\n",
    "    print(f\"- {doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833219ae",
   "metadata": {},
   "source": [
    "# İkinci Örnek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9343f8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orjinal Dokümanlar:\n",
      "- kısa bir cümle örneği\n",
      "- ikinci kısa cümle\n",
      "- başka bir örnek\n",
      "- kısa bir örnek\n",
      "\n",
      "Tokenize Edilmiş Dokümanlar:\n",
      "- kısa bir cümle örneği\n",
      "- ikinci kısa cümle\n",
      "- başka bir örnek\n",
      "- kısa bir örnek\n",
      "\n",
      "Bag of Words Matrisi:\n",
      "[[0 1 1 0 1 0 1]\n",
      " [0 0 1 1 1 0 0]\n",
      " [1 1 0 0 0 1 0]\n",
      " [0 1 0 0 1 1 0]]\n",
      "\n",
      "Özellik (Kelime) İndeksleri:\n",
      "['başka' 'bir' 'cümle' 'ikinci' 'kısa' 'örnek' 'örneği']\n",
      "\n",
      "Frekansı >= 3 olan Seçilmiş Kelimeler:\n",
      "['bir', 'kısa']\n",
      "\n",
      "Filtrelenmiş Bag of Words Matrisi:\n",
      "[[1 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizasyon ve stopwords'leri kaldırma fonksiyonu\n",
    "def tokenize_and_remove_stopwords(document):\n",
    "    words = word_tokenize(document)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Tokenize edilmiş doküman oluşturma fonksiyonu\n",
    "def tokenizedDocument(documents):\n",
    "    result = []\n",
    "    for document in documents:\n",
    "        words = tokenize_and_remove_stopwords(document)\n",
    "        result.append(' '.join(words))\n",
    "    return result\n",
    "\n",
    "# Bag of Words oluşturma fonksiyonu\n",
    "def bagOfWords(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag = vectorizer.fit_transform(documents)\n",
    "    return bag, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Seyrek kelimeleri kaldırma fonksiyonu\n",
    "def removeInfrequentWords(bag, count):\n",
    "    word_frequencies = np.array(bag.sum(axis=0))[0]\n",
    "    selected_words_indices = np.where(word_frequencies >= count)[0]\n",
    "    selected_words = [feature_names[i] for i in selected_words_indices]\n",
    "    \n",
    "    filtered_bag = bag[:, selected_words_indices]\n",
    "    \n",
    "    return filtered_bag, selected_words\n",
    "\n",
    "# Örnek cümleler\n",
    "documents = [\n",
    "    \"kısa bir cümle örneği\",\n",
    "    \"ikinci kısa cümle\",\n",
    "    \"başka bir örnek\",\n",
    "    \"kısa bir örnek\"\n",
    "]\n",
    "\n",
    "# Tokenizasyon ve stopwords'leri kaldırma\n",
    "tokenized_documents = tokenizedDocument(documents)\n",
    "\n",
    "# Bag of Words oluşturma\n",
    "bag, feature_names = bagOfWords(tokenized_documents)\n",
    "\n",
    "# Seyrek kelimeleri kaldırma\n",
    "count = 3\n",
    "newBag, selected_words = removeInfrequentWords(bag, count)\n",
    "\n",
    "# Sonuçları görüntüleme\n",
    "print(\"Orjinal Dokümanlar:\")\n",
    "for doc in documents:\n",
    "    print(f\"- {doc}\")\n",
    "\n",
    "print(\"\\nTokenize Edilmiş Dokümanlar:\")\n",
    "for doc in tokenized_documents:\n",
    "    print(f\"- {doc}\")\n",
    "\n",
    "print(\"\\nBag of Words Matrisi:\")\n",
    "print(bag.toarray())\n",
    "print(\"\\nÖzellik (Kelime) İndeksleri:\")\n",
    "print(feature_names)\n",
    "\n",
    "print(f\"\\nFrekansı >= {count} olan Seçilmiş Kelimeler:\")\n",
    "print(selected_words)\n",
    "\n",
    "print(\"\\nFiltrelenmiş Bag of Words Matrisi:\")\n",
    "print(newBag.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f9391",
   "metadata": {},
   "source": [
    "# Noktalama İşaretlerin Silinmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7484ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orjinal String: it's one and/or two.\n",
      "Noktalama Simgelerin Silinmesi: its one andor two\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def erasePunctuation(input_str):\n",
    "    # Translate fonksiyonu için çeviri tablosu oluşturma\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "    # Noktalama işaretlerini kaldırma\n",
    "    result_str = input_str.translate(translator)\n",
    "    \n",
    "    return result_str\n",
    "\n",
    "# Örnek cümle\n",
    "str_example = \"it's one and/or two.\"\n",
    "\n",
    "# Noktalama işaretlerini kaldırma işlemi\n",
    "new_str = erasePunctuation(str_example)\n",
    "\n",
    "# Sonuçları yazdırma\n",
    "print(\"Orjinal String:\", str_example)\n",
    "print(\"Noktalama Simgelerin Silinmesi:\", new_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ee5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

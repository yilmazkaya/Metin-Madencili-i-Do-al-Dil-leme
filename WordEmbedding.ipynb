{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e805ac5",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ddcdaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot Encoding Matrisi:\n",
      "[[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0]]\n",
      "Kelime İndeksleri:\n",
      "{'bu': 1, 'bir': 0, 'örnek': 22, 'dökümandır': 2, 'dökümanları': 3, 'one': 16, 'hot': 10, 'encoding': 7, 'dönüştürme': 4, 'işlemi': 12, 'için': 11, 'scikit': 17, 'learn': 15, 'kullanılıyor': 14, 'her': 9, 'kelimenin': 13, 'varlık': 18, 'veya': 20, 'yokluğunu': 21, 'gösteren': 8, 'vektör': 19, 'elde': 6, 'edilecek': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Örnek dökümanlar\n",
    "documents = [\n",
    "    \"Bu bir örnek dökümandır.\",\n",
    "    \"Örnek dökümanları one-hot encoding'e dönüştürme işlemi.\",\n",
    "    \"One-hot encoding için scikit-learn kullanılıyor.\",\n",
    "    \"Her bir kelimenin varlık veya yokluğunu gösteren bir vektör elde edilecek.\"\n",
    "]\n",
    "\n",
    "# CountVectorizer'ı kullanarak one-hot encoding'i gerçekleştirin\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "one_hot_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# One-hot encoding matrisini ekrana yazdırın\n",
    "print(\"One-hot Encoding Matrisi:\")\n",
    "print(one_hot_matrix.toarray())\n",
    "\n",
    "# Kelime indekslerini ekrana yazdırın\n",
    "print(\"Kelime İndeksleri:\")\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94f8ff",
   "metadata": {},
   "source": [
    "# CountVecdtor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "475a28d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Sayım Matrisi:\n",
      "[[1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0]\n",
      " [2 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0]]\n",
      "Kelime İndeksleri:\n",
      "{'bu': 1, 'bir': 0, 'örnek': 22, 'dökümandır': 4, 'dökümanları': 5, 'countvectorizer': 2, 'dönüştürme': 6, 'işlemi': 13, 'kullanarak': 18, 'kelime': 15, 'sayım': 21, 'matrisi': 20, 'elde': 8, 'edeceğiz': 7, 'her': 11, 'kelimenin': 16, 'döküman': 3, 'içinde': 12, 'kaç': 14, 'kez': 17, 'geçtiğini': 9, 'gösteren': 10, 'matris': 19}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Örnek dökümanlar\n",
    "documents = [\n",
    "    \"Bu bir örnek dökümandır.\",\n",
    "    \"Örnek dökümanları CountVectorizer'a dönüştürme işlemi.\",\n",
    "    \"CountVectorizer'ı kullanarak bir kelime sayım matrisi elde edeceğiz.\",\n",
    "    \"Her bir kelimenin döküman içinde kaç kez geçtiğini gösteren bir matris.\"\n",
    "]\n",
    "\n",
    "# CountVectorizer'ı kullanarak dökümanları bir kelime sayım matrisine dönüştürün\n",
    "vectorizer = CountVectorizer()\n",
    "word_count_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Matrisi ekrana yazdırın\n",
    "print(\"Kelime Sayım Matrisi:\")\n",
    "print(word_count_matrix.toarray())\n",
    "\n",
    "# Kelime indekslerini ekrana yazdırın\n",
    "print(\"Kelime İndeksleri:\")\n",
    "print(vectorizer.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53960009",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99f25ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrisi:\n",
      "[[0.         0.         0.36674667 0.57457953 0.57457953 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4530051 ]\n",
      " [0.         0.         0.         0.         0.         0.42888787\n",
      "  0.42888787 0.         0.         0.27375357 0.42888787 0.\n",
      "  0.         0.27375357 0.         0.42888787 0.         0.\n",
      "  0.         0.33814012]\n",
      " [0.31336062 0.31336062 0.25369243 0.         0.         0.\n",
      "  0.         0.         0.31336062 0.25369243 0.         0.31336062\n",
      "  0.         0.25369243 0.39745821 0.         0.         0.39745821\n",
      "  0.31336062 0.        ]\n",
      " [0.24427407 0.24427407 0.39552182 0.         0.         0.\n",
      "  0.         0.30983068 0.24427407 0.19776091 0.         0.48854814\n",
      "  0.30983068 0.19776091 0.         0.         0.30983068 0.\n",
      "  0.24427407 0.        ]]\n",
      "Kelime İndeksleri:\n",
      "{'bu': 3, 'bir': 2, 'örnek': 19, 'dökümandır': 4, 'dökümanları': 5, 'tf': 13, 'idf': 9, 'vektörlerine': 15, 'dönüştürme': 6, 'işlemi': 10, 'vektörleri': 14, 'her': 8, 'kelimenin': 11, 'belirli': 1, 'belgedeki': 0, 'önemini': 18, 'ölçer': 17, 'vektörü': 16, 'temsil': 12, 'eder': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Örnek dökümanlar\n",
    "documents = [\n",
    "    \"Bu bir örnek dökümandır.\",\n",
    "    \"Örnek dökümanları TF-IDF vektörlerine dönüştürme işlemi.\",\n",
    "    \"TF-IDF vektörleri, her kelimenin belirli bir belgedeki önemini ölçer.\",\n",
    "    \"Her bir kelimenin TF-IDF vektörü, o kelimenin belirli bir belgedeki önemini temsil eder.\"\n",
    "]\n",
    "\n",
    "# TfidfVectorizer'ı kullanarak dökümanları TF-IDF matrisine dönüştürün\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Matrisi ekrana yazdırın\n",
    "print(\"TF-IDF Matrisi:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Kelime indekslerini ekrana yazdırın\n",
    "print(\"Kelime İndeksleri:\")\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2c75f",
   "metadata": {},
   "source": [
    "# Co-Occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5499899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence Matrisi:\n",
      "[[0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0]\n",
      " [1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0]\n",
      " [1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0]\n",
      " [0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1]\n",
      " [1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1]\n",
      " [1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0]\n",
      " [1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0]\n",
      " [1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1]\n",
      " [1 1 1 1 3 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1]\n",
      " [1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1]\n",
      " [0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0]]\n",
      "Kelime İndeksleri:\n",
      "{'bu': 3, 'bir': 2, 'örnek': 21, 'dökümandır': 5, 'dökümanları': 6, 'co': 4, 'occurrence': 16, 'matrisine': 14, 'dönüştürme': 7, 'işlemi': 11, 'matrisi': 13, 'belirli': 1, 'kelimelerin': 12, 'arada': 0, 'geçme': 9, 'sıklığını': 18, 'gösterir': 10, 'örnekte': 22, 'matrisini': 15, 'oluşturuyoruz': 17, 've': 19, 'ekrana': 8, 'yazdırıyoruz': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Örnek dökümanlar\n",
    "documents = [\n",
    "    \"Bu bir örnek dökümandır.\",\n",
    "    \"Örnek dökümanları co-occurrence matrisine dönüştürme işlemi.\",\n",
    "    \"Co-occurrence matrisi, belirli kelimelerin bir arada geçme sıklığını gösterir.\",\n",
    "    \"Bu örnekte, co-occurrence matrisini oluşturuyoruz ve ekrana yazdırıyoruz.\"\n",
    "]\n",
    "\n",
    "# CountVectorizer'ı kullanarak dökümanları bir kelime sayım matrisine dönüştürün\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "word_count_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Kelime sayım matrisini co-occurrence matrisine dönüştürün\n",
    "co_occurrence_matrix = word_count_matrix.T.dot(word_count_matrix)\n",
    "\n",
    "# Diagonal değerleri 0 yapın (kelimenin kendi kendisi ile olan geçişleri)\n",
    "co_occurrence_matrix.setdiag(0)\n",
    "\n",
    "# coo_matrix'e dönüştürün\n",
    "co_occurrence_matrix = coo_matrix(co_occurrence_matrix)\n",
    "\n",
    "# Matrisi ekrana yazdırın\n",
    "print(\"Co-occurrence Matrisi:\")\n",
    "print(co_occurrence_matrix.toarray())\n",
    "\n",
    "# Kelime indekslerini ekrana yazdırın\n",
    "print(\"Kelime İndeksleri:\")\n",
    "print(vectorizer.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8de8d9",
   "metadata": {},
   "source": [
    "# CBOW\n",
    "Word2Vec sınıfını kullanarak CBOW modelini oluşturuyoruz. Parametreler şu şekildedir:\n",
    "\n",
    "sentences: Eğitim için kullanılacak cümleleri içeren liste. Burada sadece tek bir cümle kullanıyoruz.\n",
    "vector_size: Her kelimenin temsil edileceği vektör boyutu.\n",
    "window: Tahmin edilen kelimenin etrafındaki bağlamın boyutu.\n",
    "sg: Skip-gram (1) veya CBOW (0) modelini belirten parametre. Burada CBOW modelini seçtik.\n",
    "min_count: Modelde yer alacak en az sayıda kelime sayısı.\n",
    "workers: Eğitim için kullanılacak işçi sayısı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42cda11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW ile Örnek kelimenin benzer kelimeleri:\n",
      "için: 0.42765650153160095\n",
      "oluşturmak: 0.2944478392601013\n",
      "kullanılacaktır: 0.23284997045993805\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Örnek bir metin\n",
    "text = \"Bu bir örnek cümledir. CBOW modelini oluşturmak için kullanılacaktır.\"\n",
    "\n",
    "# Metni tokenlara ayırın\n",
    "tokens = word_tokenize(text.lower())  # Küçük harfe çevirme işlemi\n",
    "\n",
    "# CBOW modelini oluşturun\n",
    "model_cbow = Word2Vec(sentences=[tokens], vector_size=10, window=3, sg=0, min_count=1, workers=4)\n",
    "\n",
    "# Modeli eğitin\n",
    "model_cbow.train([tokens], total_examples=1, epochs=10)\n",
    "\n",
    "# Kelimeler arasındaki benzerlikleri kontrol edin\n",
    "similar_words_cbow = model_cbow.wv.most_similar('örnek', topn=3)\n",
    "\n",
    "# Sonuçları yazdırın\n",
    "print(\"CBOW ile Örnek kelimenin benzer kelimeleri:\")\n",
    "for word, score in similar_words_cbow:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075dbad",
   "metadata": {},
   "source": [
    "# skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95eb0d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram ile Örnek kelimenin benzer kelimeleri:\n",
      "skip-gram: 0.2914133667945862\n",
      "için: 0.05529548600316048\n",
      "modelini: 0.04264767840504646\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Örnek bir metin\n",
    "text = \"Bu bir örnek cümledir. Skip-gram modelini oluşturmak için kullanılacaktır.\"\n",
    "\n",
    "# Metni tokenlara ayırın\n",
    "tokens = word_tokenize(text.lower())  # Küçük harfe çevirme işlemi\n",
    "\n",
    "# Skip-gram modelini oluşturun\n",
    "model_skipgram = Word2Vec(sentences=[tokens], vector_size=10, window=3, sg=1, min_count=1, workers=4)\n",
    "\n",
    "# Modeli eğitin\n",
    "model_skipgram.train([tokens], total_examples=1, epochs=10)\n",
    "\n",
    "# Kelimeler arasındaki benzerlikleri kontrol edin\n",
    "similar_words_skipgram = model_skipgram.wv.most_similar('bu', topn=3)\n",
    "\n",
    "# Sonuçları yazdırın\n",
    "print(\"Skip-gram ile Örnek kelimenin benzer kelimeleri:\")\n",
    "for word, score in similar_words_skipgram:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827f2ef",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d86fe03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime Vektörü (örnek): [ 0.01624836  0.00190858  0.03468794  0.00220425  0.09621963  0.05060079\n",
      " -0.08915306 -0.07035083  0.00899166  0.06398909]\n",
      "Örnek kelimenin benzer kelimeleri:\n",
      "kullanarak: 0.2915043830871582\n",
      "vektörlerini: 0.05535745620727539\n",
      "kelime: 0.0433088056743145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Örnek bir metin\n",
    "text = \"Bu bir örnek cümledir. Word2Vec modelini kullanarak kelime gömme vektörlerini öğreniyoruz.\"\n",
    "\n",
    "# Metni tokenlara ayırın\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Word2Vec modelini oluşturun\n",
    "model = Word2Vec(sentences=[tokens], vector_size=10, window=3, sg=1, min_count=1, workers=4)\n",
    "\n",
    "# Modeli eğitin\n",
    "model.train([tokens], total_examples=1, epochs=10)\n",
    "\n",
    "# \"örnek\" kelimesinin vektörünü alın\n",
    "vector_example = model.wv['örnek']\n",
    "\n",
    "# Benzer kelimeleri kontrol edin\n",
    "similar_words = model.wv.most_similar('örnek', topn=3)\n",
    "\n",
    "# Sonuçları yazdırın\n",
    "print(\"Kelime Vektörü (örnek):\", vector_example)\n",
    "print(\"Örnek kelimenin benzer kelimeleri:\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff610284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

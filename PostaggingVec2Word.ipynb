{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fdd997",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e401813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Token  PoS\n",
      "0   How  WRB\n",
      "1   are  VBP\n",
      "2   you  PRP\n",
      "3     ?    .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "#Şu satırı yorum satırı yapın eğer nltk veri setlerini yüklediyseniz\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metin = \"How are you?\"\n",
    "\n",
    "# Metni satırlara bölen işlem\n",
    "textData = metin.splitlines()\n",
    "\n",
    "# Tokenization işlemi\n",
    "tokenlar = [word_tokenize(satir) for satir in textData]\n",
    "\n",
    "# Part of Speech (PoS) detayları ekleyerek tokenları işleme\n",
    "tokenlar_pos = [pos_tag(token) for token in tokenlar]\n",
    "\n",
    "# Token ayrıntıları oluşturma\n",
    "token_ayrinti = pd.DataFrame([(token, pos) for cumle in tokenlar_pos for token, pos in cumle],\n",
    "                              columns=['Token', 'PoS'])\n",
    "\n",
    "# İlk birkaç satırı görüntüleme\n",
    "print(token_ayrinti.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ff406",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4986e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benzer Kelimeler\n",
      "train: 0.1281471848487854\n",
      "will: 0.10943187028169632\n",
      "english: 0.10889625549316406\n",
      "the: 0.09615458548069\n",
      ".: 0.0628497302532196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')  # Tokenization için gerekli olan punkt veri setini indirin\n",
    "\n",
    "# Örnek İngilizce metin verisi\n",
    "english_text = \"Hello, this is an example English text. We will train a Word2Vec model using the Gensim library.\"\n",
    "\n",
    "# Metni tokenlara ayırma\n",
    "tokenized_text = word_tokenize(english_text.lower())  # Küçük harfe çevir ve tokenlara ayır\n",
    "\n",
    "# Word2Vec modelini eğitme\n",
    "model = Word2Vec([tokenized_text], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Modeli kaydetme\n",
    "model.save(\"word2vec_model_en.bin\")\n",
    "\n",
    "# Örnek kullanım\n",
    "similar_words = model.wv.most_similar('gensim', topn=5)\n",
    "print(\"Benzer Kelimeler\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075bdd1",
   "metadata": {},
   "source": [
    "# Name Entity Recognation (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b8c7251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc.: ORG\n",
      "Steve Jobs: PERSON\n",
      "Steve Wozniak: PERSON\n",
      "California: GPE\n",
      "April 1, 1976: DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# spaCy'den İngilizce dil modelini yükleyin\n",
    "#Önce bilgisayara bu dosyanın yüklenmiş olması gerek \n",
    "#python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Örnek metin\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in California on April 1, 1976.\"\n",
    "\n",
    "# Metni işleyin\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tanımlanan varlıkları görüntüleyin\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bc420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
